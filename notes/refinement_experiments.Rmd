# Experiments with refinements

```{r workflow-demo-libraries}

# Libraries to attach
library("tidyverse")
library("slam")
library("Barycenter")
library("MCMCpack")
library("dplyr")
library("embed")
library("ggplot2")
library("ggraph")
library("ggrepel")
library("magrittr")
library("pdist")
library("purrr")
library("stringr")
library("tidygraph")
library("tidymodels")
library("tidyr")
library("topicmodels")
library("topicvis")
library("cowplot")
source("refinement_functions.R")
## load the topic alignment functions
source("../align_topic_functions.R")
## viz default theme
theme_set(theme_minimal())

```

## Refinements on k-means clustering data

I want to start off with some simulations where it's really easy to see what's going on so that we can gain better intuition about the things we're estimating.
I'm going to use the simulations that Kris set up in a previous repo.
There are two different situations here: the first is a set of four very distinct clusters, and the second is just a set of draws from a bivariate normal distribution.
We can see them below, along with the $k$-means solutions for $k$ between 2 and 8.

```{r, fig.width=8, fig.height=6,dpi=200}
set.seed(0)
n <- 400
mu0 <- c(0, 0)
mu1 <- c(5, 5)
x <- list()
x$smooth <- simulate_gradient(n, mu0, mu1)
mus <-  matrix(rnorm(10, sd = 10), ncol = 2)
x$cluster <- simulate_cluster(n, mus)
links <- list(
  "cluster" = linked_cluster_data(x$cluster, Ks = 1:8),
  "smooth" = linked_cluster_data(x$smooth, Ks = 1:8)
)
p1 <- ggplot(links$cluster$centroids) +
  geom_point(data = data.frame(x$cluster), aes(x = X1, y = X2), size = 0.2) +
  geom_point(aes(x = X1, y = X2), col = "red", size = 1) +
  coord_fixed() +
  facet_wrap(~K)

p2 <- ggplot(links$smooth$centroids) +
  geom_point(data = data.frame(x$smooth), aes(x = X1, y = X2), size = 0.2) +
  geom_point(aes(x = X1, y = X2), col = "red", size = 1) +
  coord_fixed() +
  facet_wrap(~K)
plot_grid(p1, p2)
```

Once we have the clustering solutions in each case, we can look at what the estimates for refinements are.
For now, I'm just using least squares to estimate refinements instead of something fancier.
That is, if $C_i$ is an indicator matrix of samples x clusters, then the refinement for $C_1$ and $C_2$ is estimated as being the matrix $R$ that minimizes $\| C_1 - C_2 R\|_2^2$.
I haven't written it out, but it seems that with this way of estimating $R$, we always get $\hat R$ with row sums equal to 1.
For hard clustering, I've also always had $R_{ij} \in [0,1]$ (this hasn't been true for the soft clustering that we have for LDA).

We can look at what happens with the refinements when we try it on the cluster-simulated data.
We see that in this case, we usually see that $R$ is a matrix with only 0's and 1's, and it corresponds exactly to the refinement structure that we see by eye.
```{r}
make_refinements_from_cluster_data = function(cluster_data, k1, k2) {
    C1 = cluster_data$groups %>%
        round() %>%
        subset(K == k1) %>%
        pivot_wider(names_from = cluster, id_cols = ix, values_from = K, values_fill = 0, values_fn = function(x) 1)
    C2 = cluster_data$groups %>%
        round() %>%
        subset(K == k2) %>%
        pivot_wider(names_from = cluster, id_cols = ix, values_from = K, values_fill = 0, values_fn = function(x) 1)
    C1_m = as.matrix(select(C1, -"ix"))
    C2_m = as.matrix(select(C2, -"ix"))

    ## refinement matrix estimated by least squares
    R = solve(t(C2_m) %*% C2_m) %*% t(C2_m) %*% C1_m
    fit = C2_m %*% R
    rmse = sqrt(mean((C1_m - fit)^2))
    return(list(R = R, rmse = rmse))
}

make_refinements_from_cluster_data(links$cluster, 2, 3)
make_refinements_from_cluster_data(links$cluster, 2, 4)
make_refinements_from_cluster_data(links$cluster, 4, 7)
```

An exception is when we try this for clustering solutions that aren't refinements of each other, for example the solutions $k = 5$/$k = 6$ or $k = 6$/$k = 7$.
In this case, we get an $\hat R$ that is not all 0's and 1's.
We also see that the least squares error is substantially higher than in the other two cases.
```{r}
make_refinements_from_cluster_data(links$cluster, 5, 6) %>% lapply(round, digits = 2)
make_refinements_from_cluster_data(links$cluster, 6, 7) %>% lapply(round, digits = 2)
```

Now we can move on to the smooth-simulated data and see what happens.
I was a little surprised to see here that we often see numbers close to 1/0 (see $k = 3$/$k = 4$ for example, not just the .99/.97 but the .87/.13).
In retrospect this makes sense, 
```{r}
make_refinements_from_cluster_data(links$smooth, 2, 3) %>% lapply(round, digits = 2)
make_refinements_from_cluster_data(links$smooth, 3, 4) %>% lapply(round, digits = 2)
make_refinements_from_cluster_data(links$smooth, 3, 5) %>% lapply(round, digits = 2)
```

### Quantifying a refinement "path"

We talked about quantifying a path through a series of clusters instead of just quantifying one cluster to another.
To me, the most natural way to do this is to multiply the weights for each segment.
The notation gets very ugly, but suppose that $R^{i \to j}$ is the refinement matrix describing the transition from clustering solution $i$ to clustering solution $j$.
Then if we want to compute the weight for a path of clusters: cluster $c_1$ in clustering solution $\mathcal C_1$ to cluster $c_2$ in clustering solution $\mathcal C_2$ to $\ldots$ to cluster $c_n$ in clustering solution $\mathcal C_n$, we write
$$
\prod_{i=1}^{n-1} R^{\mathcal C_i \to \mathcal C_{i+1}}_{c_{i+1}, c_i}
$$

This has a reasonable interpretation: if we think of the set of refinements as defining a Markov process where the probability of going from cluster $c_i$ in solution $\mathcal C_i$ to cluster $c_{i+1}$ in $\mathcal C_{i+1}$ is given by the relevant entry in $R^{\mathcal C_i \to \mathcal C_{i+1}}$, then the weight for the path is the same as the probability that a Markov process starting at $c_1$ will follow exactly the specified path.

We can see what this does on our $k$-means data.

```{r}
make_refinement_sequence = function(cluster_data, k_sequence) {
    refinement_list = lapply(k_sequence[-length(k_sequence)], function(k)
        make_refinements_from_cluster_data(cluster_data, k, k+1)$R)
    return(refinement_list)
}
get_path_weights = function(cluster_data, cluster_sequence, k_sequence) {
    refinement_list = make_refinement_sequence(cluster_data, k_sequence)
    weights = numeric(length(k_sequence) - 1)
    for(i in 1:(length(cluster_sequence) - 1)) {
        c1 = cluster_sequence[i]; c2 = cluster_sequence[i+1]
        weights[i] = refinement_list[[i]][c2, c1]        
    }
    return(prod(weights))
}
```

What it looks like on the smooth data:
```{r}
make_refinement_sequence(links$smooth, k_sequence = seq(2,5))
get_path_weights(links$smooth, c('1', '3', '3', '5'), k_sequence = seq(2,5))
get_path_weights(links$smooth, c('1', '3', '3', '2'), k_sequence = seq(2,5))
```

And on the clustered data:
```{r}
make_refinement_sequence(links$cluster, k_sequence = seq(2, 6))
get_path_weights(links$cluster, c('1', '2', '2', '5', '5'), k_sequence = seq(2, 6))
get_path_weights(links$cluster, c('2', '1', '4', '3', '2'), k_sequence = seq(2, 6))
```

## Cluster split scores

Let's make a function that computes a score for one cluster splitting into multiple clusters.

```{r}
##' @param cluster_id is the cluster id within one clustering solution
##' @param k is the value of k for the clustering solution we find cluster_id in
##' @return A vector of scores. All 1's means that all the descendants of cluster_id are refinements of the children of cluster_id, and so splitting cluster_id into its children is probably a good idea. Scores lower than 1 mean that this isn't true, and the split is more likely to be arbitrary.
get_split_score = function(cluster_data, k, cluster_id, k_max) {
    R12 = make_refinements_from_cluster_data(cluster_data, k, k+1)$R
    daughters = which(R12[,cluster_id] >= .99)
    daughter_names = rownames(R12)[daughters]
    scores = numeric(k_max - k - 1)
    for(j in (k+2):k_max) {
        R1j = make_refinements_from_cluster_data(cluster_data, k, j)$R
        R2j = make_refinements_from_cluster_data(cluster_data, k+1, j)$R
        descendants = which(R1j[,cluster_id] >= 0)
        descendant_names = rownames(R1j)[descendants]
        daughter_descendant_scores = R2j[descendant_names,daughter_names,drop = FALSE]
        ## OR logic = min, we want everything to be a refinement
        ## abs(.5 - x) is a function that is maximal when x is 0 or 1 (indicating a refinement) and minimal when x = .5 (indicating not a refinement)
        scores[j - k - 1] = min(2 * abs(.5 - daughter_descendant_scores))
    }
    return(mean(scores))
}
get_split_score(links$cluster, k = 2, cluster_id = '2', k_max = 8)
## 4 doesn't split when we go from k = 4 to k = 5
get_split_score(links$cluster, k = 4, cluster_id = '4', k_max = 8)
## 1 splits into 4 and 5, which it should not
get_split_score(links$cluster, k = 4, cluster_id = '1', k_max = 8)
get_split_score(links$cluster, k = 5, cluster_id = '3', k_max = 8)
get_split_score(links$cluster, k = 3, cluster_id = '3', k_max = 8)
get_split_score(links$smooth, k = 1, cluster_id = '1', k_max = 8)
get_split_score(links$smooth, k = 2, cluster_id = '2', k_max = 8)
```

Use the simulation functions from Kris's script to make data and fit with LDA:

```{r}
source("../align_topic_functions.R")
source("../simulation_functions.R")
n_lev <- 3
tree <- generate_tree(n_lev)
topics <- tree_topics(tree)
leaves <- cbind(tree, topics) %>%
  filter(leaf) %>%
  dplyr::select(`1`:`10`)
n <- 100
gamma <- rdirichlet(n, alpha = rep(0.1, nrow(leaves)))
colnames(gamma) <- seq(2 ^ n_lev, 2 ^ (n_lev + 1) - 1)
x <- simulate_lda(leaves, gamma)
fits <- run_lda_models(x, 1:2^n_lev, c(.1), "VEM", 123, reset = TRUE)
alignment <- align_topics(fits)
visualize_aligned_topics(alignment)
```

The split score for $k = 1$ to $k = 2$. It's quite low, which is in line with the visualization: the two clusters we get when $k = 2$ are not consistently the unique parents of the later clustering solutions.
```{r}
get_split_score_lda(fits, k = 1, cluster_id = 'topic_a', k_max = 8)
```

We can try the same thing with more distinct clusters:

```{r}
n_lev <- 3
tree <- generate_tree(n_lev, lambda = .001)
topics <- tree_topics(tree)
leaves <- cbind(tree, topics) %>%
  filter(leaf) %>%
  dplyr::select(`1`:`10`)
n <- 100
gamma <- rdirichlet(n, alpha = rep(0.1, nrow(leaves)))
colnames(gamma) <- seq(2 ^ n_lev, 2 ^ (n_lev + 1) - 1)
x <- simulate_lda(leaves, gamma)
fits <- run_lda_models(x, 1:2^n_lev, c(.1), "VEM", 123, reset = TRUE)
get_split_score_lda(fits, k = 1, cluster_id = 'topic_a', k_max = 8)
get_split_score_lda(fits, k = 2, cluster_id = 'topic_a', k_max = 8, daughter_threshold=.7)
get_split_score_lda(fits, k = 3, cluster_id = 'topic_a', k_max = 8, daughter_threshold=.7)
```

```{r}
alignment <- align_topics(fits)
visualize_aligned_topics(alignment)
```

## Refinements for the VM data


We can load and fit LDAs on the VM data using Laura's code:

```{r workflow-demo-load-and-prepare-data}
load(file = "../vm_16s_data.Rdata", verbose = TRUE)
new_asv_names =  colnames(vm_16s) %>% 
  str_split_fixed(., " ", n = 8) %>%
  as.matrix() %>% .[,c(6, 7, 8)]  %>%
  as.data.frame() %>% 
  set_colnames(c("genus","species","strain")) %>% 
   mutate(short_name = 
            str_c(genus, " ", 
                  species %>% str_replace(.,"NA","-")," ",
                  strain)) %>% 
  select(short_name) %>% unlist()

j = which(duplicated(new_asv_names))
new_asv_names[j] = str_c(new_asv_names[j], " (", 1:length(j),")")
colnames(vm_16s) = new_asv_names
  
vm_16s <- slam::as.simple_triplet_matrix(vm_16s %>%  round())
topic_models_dir = "../lda_models/"
lda_models = 
  run_lda_models(
    data = vm_16s,
    Ks = 1:13,
    method = "VEM",
    seed = 2,
    dir = topic_models_dir
  )
```

We can look at an example of a refinement estimated by least squares:
```{r}
k1 = 5; k2 = 6
C1 = lda_models$gammas %>%
    subset(K == k1) %>%
    pivot_wider(id_cols = d, names_from = k_LDA, names_prefix = "topic_", values_from = g)

C2 = lda_models$gammas %>%
    subset(K == k2) %>%
    pivot_wider(id_cols = d, names_from = k_LDA, names_prefix = "topic_", values_from = g)

mean(C1$d == C2$d)
C1_m = as.matrix(C1[,-1])
C2_m = as.matrix(C2[,-1])

## refinement matrix estimated by least squares
R = solve(t(C2_m) %*% C2_m) %*% t(C2_m) %*% C1_m
R %>% round(digits = 2)
```

We can also check to see whether it seems like putting box constraints on $\hat R$ will help very much.
It doesn't look like it to me.
```{r refinement-cvx}
library(CVXR)
Rhat = Variable(k2, k1)
objective = Minimize(sum((C1_m - C2_m %*% Rhat)^2))
problem = Problem(objective, constraints = list(Rhat >= 0, Rhat <= 1, Rhat %*% rep(1, ncol(Rhat)) == 1))
result = solve(problem)
round(result$getValue(Rhat), digits = 2)
```
