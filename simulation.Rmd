---
title: "Simulations"
output:
  pdf_document:
    template: notes/style.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, message = FALSE, warning = FALSE, fig.width = 5, fig.height = 2)
```

\begin{enumerate}
\item The purpose of simulation is to provide a controlled environment within
which we can compare alternative analysis strategies. At the very least, we can
define a distance between learned alignments and specify which choices lead to
similar and different alignments. Ideally, we could also say something about
whether one alignment is ``better'' than another, but I'm not worried if we
can't answer this definitively.
\item Strategy 1 [Ordinary LDA]: We could just simulate from an LDA model with a
fixed $K$. This sounds a little silly -- why bother with alignment? -- but the
truth is we'd never know $K$ in practice, so the alignment exercise could still
be productive. This simulation is also useful if we want to illustrate how
refinements (and their associated metrics) look before and after the true $K$ is
hit.
\item Something I don't like about just simulating from a fixed $K$ is that
people might then become fixated on choosing the ``right'' $K$, which is
inimical\footnote{To use a word I just learned.} to the whole point of the
study. I'd like to argue that none of the choices of $K$ are true, but for a
given purpose, there might be a few ranges of $K$ where results are similar.
\end{enumerate}

# Strategy 2: Branching $\beta$'s
\begin{enumerate}
\item For this reason, a more complex setup might be useful. The idea in this
strategy is to simulate topics that branch off from one another over time. We
would simulate documents from the topics at the leaves. However, since we know
the overall branching structure, we can argue that topics with a similar
most-recent-common-ancestor should appear closer to one another in our diagram.
Further, it would be interesting if the intermediate branches on the tree are
somehow recovered when estimating with smaller $K$.
\item How to implement strategy 2, specifically? We can define something
inductively. Start with a $\mu_{0} = 0$. Given a tree of means, we simulate a
new leaf node according to
\begin{align*}
\mu_{i} \vert \mu_{\mathcal{P}\left(i\right)} &\sim \Gsn\left(\mu_{i} \vert \mu_{\mathcal{P}\left(i\right)}, T_{\mathcal{P}\left(i\right)} I_{d}\right)
\end{align*}
where $T_{\mathcal{P}\left(i\right)}$ denotes the time elapsed between the
parent node $\mathcal{P}\left(i\right)$ and all its children.
\item To fully specify this, we need to determine (i) the $T_i$'s and (ii) the
number of children for each node. For (i), we just simulate $T_i \sim
\text{Expo}\left(\lambda\right)$, and for (ii) we just always choose 2. \item
This is a simulation in $\reals^{V}$, but our data have to lie on $\simplex^{V -
1}$. So, let's squash all the means onto the simplex. Specifically, suppose $L$
indexes all the leaf nodes. Then we define topics according to,
\begin{align*}
\beta_{l} := \frac{\exp{\mu_{l}}}{\sum_{l^{\prime} \in L} \exp{\mu_{l^{\prime}}}},
\end{align*}
where $\exp{}$ is applied elementwise.
\end{enumerate}

# What does strategy 2 look like?

```{r}
library("Barycenter")
library("MCMCpack")
library("dplyr")
library("embed")
library("ggplot2")
library("ggraph")
library("ggrepel")
library("magrittr")
library("pdist")
library("purrr")
library("stringr")
library("tidygraph")
library("tidymodels")
library("tidyr")
library("topicmodels")
source("align_topic_functions.R")
source("simulation_functions.R")
set.seed(123)
mytheme <- theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "#f7f7f7"),
    panel.border = element_rect(fill = NA, color = "#0c0c0c", size = 0.6),
    legend.position = "bottom"
  )
theme_set(mytheme)
```

```{r}
n_lev <- 3
tree <- generate_tree(n_lev)
topics <- tree_topics(tree)
```

```{r, fig.cap = "A simulated tree. Larger jumps in time will lead to larger differences between parent and child topics."}
ggplot(tree %>% mutate(m = as.numeric(m)), aes(x = m, y = time)) +
  geom_segment(aes(xend = m - 1, yend = parent_time), col = "#d3d3d3") +
  geom_point() +
  geom_text_repel(aes(label = node))
```

```{r, fig.cap = "Topics associated with tree nodes, at the three simulated levels (plus root). Notice that neighboring rows tend to have similar b[k,v]'s.", echo = FALSE, fig.height = 3, fig.width = 4}
cbind(tree, topics) %>%
  pivot_longer(cols = c(`1`:`10`), names_to = "v") %>%
  ggplot() +
  geom_tile(aes(x = v, fill = value, y = as.factor(node)), stat = "identity") +
  facet_grid(m ~ ., scale = "free_y", space = "free") +
  labs(x = "word", y = "Tree Node") +
  scale_y_discrete(expand = c(0, 0)) +
  scale_x_discrete(expand = c(0, 0)) +
  scale_fill_distiller(direction = 1) +
  theme(
    panel.spacing=unit(.05, "lines"),
    panel.border = element_rect(color = "black", fill = NA, size = 1),
    strip.background = element_rect(color = "black", size = 1),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )
```

# Counts from Branching $\beta$'s

\begin{enumerate}
\item Given these $\beta$'s, we can simulate a dataset of counts $\*X\in
\reals^{n \times V}$. We simulate $\gamma_{i} \sim \text{Dir}\left(\alpha
\*1_{K}\right)$ and then draw $\*x_{i} \sim \text{Mult}\left(n_{0},
B\gamma_{i}\right)$ where $B \in \reals^{V \times K}$ concatenates all the
topics horizontally.
\item A larger $\alpha$ creates $\gamma_{i}$'s across the full polytope whose
corners are $\beta_{k}$. Smaller $\alpha$ encourages $\gamma_{i}$ to concentrate
at corners or edges of this polytope. The choice of $\alpha$ basically
interpolates between gradient and clustering structure. Since it seems easier to
recover hierarchical structure in the more clustered setting, we choose $\alpha
= 0.1$.
\end{enumerate}

```{r}
leaves <- cbind(tree, topics) %>%
  filter(leaf) %>%
  dplyr::select(`1`:`10`)
n <- 100
gamma <- rdirichlet(n, alpha = rep(0.1, nrow(leaves)))
colnames(gamma) <- seq(2 ^ n_lev, 2 ^ (n_lev + 1) - 1)
x <- simulate_lda(leaves, gamma)
fits <- run_lda_models(x, 2 ^ seq(0, n_lev), c(.1), "VEM", 123, reset = TRUE)
```

```{r, fig.cap = "Example fitted topics, from the branching scenario.", echo = FALSE, fig.width = 4, fig.height = 3}
fits$betas$k_LDA_ <- as.factor(fits$betas$k_LDA)
ggplot(fits$betas, aes(x = w, y = reorder(k_LDA_, desc(k_LDA_)), fill = b)) +
  geom_tile() +
  facet_grid(m ~ ., scale = "free", space = "free") +
  labs(x = "word", y = "topic") +
  scale_y_discrete(expand = c(0, 0)) +
  scale_x_discrete(expand = c(0, 0)) +
  scale_fill_distiller(direction = 1) +
  theme(
    panel.spacing=unit(.05, "lines"),
    panel.border = element_rect(color = "black", fill = NA, size = 1),
    strip.background = element_rect(color = "black", size = 1),
    axis.ticks.y = element_blank()
  )
```
```{r, fig.cap = "Alignment when using estimated topics.", fig.width = 4, fig.height = 2.5}
alignment <- align_topics(fits)
visualize_aligned_topics(alignment)
```

```{r}
simulate_replicate <- function() {
  x <- simulate_lda(leaves, gamma)
  fits <- run_lda_models(x, 2 ^ seq(0, n_lev), c(.1), "VEM", 123, reset = TRUE)
  endpoint_topics(fits$beta, alignment$gamma_alignment)
}
  
edges <- map_dfr(seq_len(20), ~ simulate_replicate(), .id = "replicate")
edge_fits <- embed_edges(edges)
```


```{r, fig.height = 5, fig.width = 5}
edge_scores <- juice(edge_fits) %>%
  mutate(endpoints = str_c(k_LDA, k_LDA_next, sep = "_")) %>%
  group_by(K, endpoints) %>%
  mutate(
    mean_weight = mean(edge_weight), 
    mean_umap_1 = mean(umap_1), 
    mean_umap_2 = mean(umap_2)
  ) %>%
  group_by(replicate, K, endpoints) %>%
  slice_head()

edge_labels <- edge_scores %>%
  group_by(K, endpoints) %>%
  slice_head()

ggplot(edge_scores, aes(umap_1, umap_2, col = K)) +
  geom_segment(aes(xend = mean_umap_1, yend = mean_umap_2), col = "black", size = 0.1, alpha = 0.3) +
  geom_point(aes(size = edge_weight), alpha = 0.2) +
  geom_point(aes(mean_umap_1, mean_umap_2), alpha = 1) +
  geom_text_repel(
    data = edge_labels %>% filter(mean_weight > 5e-4),
    aes(label = endpoints),
    size = 3
  ) +
  scale_color_brewer(palette = "Set2") +
  scale_size(range = c(0.1, 6)) +
  coord_fixed()
```


\begin{enumerate}
\item It's not clear how to compare the fitted with the true set of topics,
because we would need to account for two sources of error: failure to recover
the correct topics or failure to align. It would be simpler if we could decouple
these different sources of error.
\item For the second, we could align the true topics as they are, to see if they
recover the tree. This is easy to understand, but leaves us open to the
criticism that alignment might collapse if there are even small errors in
recovered topics.
\item For the first, we could compute the likelihood of new data under the true
and recovered topics\footnote{We would have to identify reasonable choices of
$\gamma_{i}$ in the new data, perhaps regressing documents onto topics?}. \item
To get towards the second item above, we will ignore `x` for now, focusing
solely on the simulated topic tree. I arbitrarily set all the topic weights
equal to one another.
\end{enumerate}

\begin{enumerate}
\item It seems that for the tree constructed above, it's easy to reconstruct the
alignment. Is there a point at which it breaks down? A more challenging
situation will be necessary in order to fruitfully compare competing approaches.
\item One way to make the problem harder is to give less time for the topics to
evolve along the tree. This can be accomplished by reducing the mean of the
exponential branching times. It would be interesting to estimate these
alignments along a sequence of increasingly challenging problems, in the spirit
of Le Cam's LAN.
\item This intuition is confirmed in the figure below.
\end{enumerate}

```{r, fig.width = 4, fig.height = 2.5, fig.cap = "Topic alignments deteriorate when the amount of time given to evolve is reduced."}
tree <- generate_tree(n_lev, lambda = 1000)
topics <- tree_topics(tree)
```


# Multimodal simulation

\begin{enumerate}
\item How can we evaluate alignment of topics learned across modalities?
\item Strategy 1: We can simulate $\gamma$'s across modes so that some
``columns'' are shared between modes while others are unique. The $\beta$'s can
be completely independent, since we don't really care to compare them anyways.
\item To elaborate, choose $K_{X}, K_{Y}$, and $K_{S}$ for the number of topics
that are unique to $X$ and $Y$ and shared ($S$). For the first $K_{S}$, we can
simulate mixed memberships $\gamma_{k}^{X} \equiv \gamma_{k}^{Y}$. For the
remaining $K_{X}$ topics in $X$, we simulate $\gamma_{k}^{X}$ without regard to
$Y$ (and vice versa).
\item How can we tie together the $\gamma_{k}$'s for shared indices? A simple
approach is to use a stick breaking construction. For $k \leq K_{S}$, we
simulate,
\begin{align*}
\gamma_{k} &:= v_{k}\prod_{k^{\prime} < k} \left(1 - v_{k^{\prime}}\right) \\
v_{k} &\sim \text{Beta}\left(1, \alpha\right),
\end{align*}
and we set $\gamma_{k}^{X} = \gamma_{k}^{Y} = \gamma_{k}$. For the remaining
topics within the $X$ modality, we draw from
\begin{align*}
\gamma_{k} &:= v_{k}\prod_{k^{\prime} = 1}^{K_{S}}\left(1 - v_{k^{\prime}}\right)\prod_{k^{x} = K_{S} + 1}^{k - 1}\left(1 - v_{k^{x}}\right) \\
v_{k^{x}} &\sim \text{Beta}\left(1, \alpha\right),
\end{align*}
and similarly for mode $Y$.
\item The interpretation of the process above is that we take a stick and break
off $K_{S}$ parts, whose lengths correspond to the $\gamma_{k}$'s for the shared
topics. We then clone whatever is left of the stick, so there is one stick per
each mode. Those remaining sticks are then broken independently via the same
mechanism.
\item A downside of Strategy 1 is that it doesn't evaluate our ability to search
for ``soft'' matchings between topics across modes. The shared topics in one
mode can be perfectly matched to the shared topics in the other.
\end{enumerate}

```{r}
gammas <- multimodal_gammas(100, c(3, 3),c(3, 3), k_shared = 4, alpha_shared = 3)
map(gammas, head)
```

```{r}
source <- list(pos = gammas[[1]], mass = rep(1, ncol(gammas[[1]])))
target <- list(pos = gammas[[2]], mass = rep(1, ncol(gammas[[2]])))
transport_align_pair(source, target, lambda = 0.01)
```
