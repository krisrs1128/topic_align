---
title: "Aligning topics"
author: "Kris, Laura & Julia"
date: "1/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
```

# Objectives

We aim at developing a method to align and compare topics when

(1) the number of topics is changed (varying K)

(2) hyper-parameters of LDA are changed (e.g. varying alpha)

(3) different modalities [? is there a better term than "modality" ?] exist for the same documents. For example, the same set of documents exists in different languages but we don't have a direct translation of each word. Or, more commonly encountered in biology, the same samples have been analyzed for different -omics information, _e.g._ metagenomic or transcriptomic, and there is a desire to compare the topics from these different domains.

(4) different datasets composed of different documents exist with the same modality, i.e. the same words are present in both datasets. For example, one dataset is the collection of articles from The New York Times and the other dataset is the collection of articles from The Guardian and we wish to compare the topics identified in these two datasets. In microbiome biology, we may want to compare topics obtained from two different cohorts. For example, the vaginal microbiome composition of pregnant (cohort 1) and non-pregnant (cohort 2) women has been measured by rRNA 16S sequencing and we want to compare the communities identified in these two cohorts.


# Notations

__Models__

We aim to compare the topics of $M$ LDA models. Each specific model is denoted by $m \in [1:M]$.

__Topics__

Each model $M$ has $K$ topics. Each topic is denoted by $k \in [1:K]$.

__Documents (Samples)__

The dataset is composed of $D$ documents (or samples). Each document/sample is denoted by $d \in [1:D]$. 

__Words (features)__

The dataset contains counts for a set of $W$ words (or features). In biology, these features would be genes, transcripts, proteins, bacterial species, etc. Each word is denoted by an index $w \in [1:W]$. The number of word $w$ found in a specific document $d$ is denoted by $c_{w,d}$.

__LDA model matrices__

LDA models are defined by two matrices: 

- $\beta$, which is a $K \times W$ matrix where element $\beta_{k,w}$ provides the proportion of word $w$ in topic $k$, and 

- $\gamma$, which is a $D \times K$ matrix where element $\gamma_{d,k}$ provides the proportion of topic $k$ in document $d$.

In order words, an LDA finds topics such that each document is optimally described as a mixture of topics ($\gamma$), themselves characterized by a word probability ($\beta$).


# Aligning topics.

For objectives (1) and (2), we can align topics either using the $\beta$ or the $\gamma$ matrices from each model $m$. For objective (3), only matrix $\gamma$ can be used to align topics and for objective (4) only matrix $\beta$ can be used.

We will first consider the problem of aligning topics using the $\gamma$ matrices, then consider the twin problem of aligning topics using the $\beta$ matrices and discuss similarities and differences.

In both case, in addition to aligning topics between successive models (_e.g._ successive values of K or $alpha$, or manually ordered modalities or samples), we are also interested in computing and visualizing the alignment between each model.

## Aligning topics based on the $\gamma$ matrices

The alignment based on the $\gamma$ matrices is a simpler problem than the alignment based on the $\beta$ matrices because we can assign a mass to each document and compute how the mass of each document is transferred between topics of a first model and topics of a second model, since the $\gamma$ elements can be interpreted as the distribution of mass from each document between the topics.

To compute the alignment based on the $\gamma$ matrices, we simply compute the proportion of mass transferred between each topic of successive models as $w^{\gamma}_{k^m, k^{m+1}} = \frac{1}{D} \sum_{d}^D \gamma_{d,k^m} \ \gamma_{d, k^{m+1}}$.

Consequently, the "height" (or total mass) of each topic $h_{k^m}$ is $h_{k^m} = \sum_d \gamma_{d,k^m}$. Topics that are the main topics of many documents have a larger "height" that topics that are secondary topics of many documents or the main topic of few documents.

And if we desire to split these weights by topics of a reference model, 
we can assign each document $d$ to topic of reference $k_R$. This topic of reference is defined as the topic of the reference model $m_R$ with the largest proportion for this document: $k^R_d = \arg \max \gamma_{d,k^R}$.

Then, the alignment becomes $w^{\gamma}_{k^m, k^{m+1},k^R} = \frac{1}{D^R} \sum_d^{D^R} \ \gamma_{d,k^m} \ \gamma_{d, k^{m+1}} $.


## Aligning topics based on the $\beta$ matrices

Aligning topics based on the $\beta$ matrices is more complex as it requires to optimize (instead of merely compute) the mass transfer between topics. That is because there is no clear concept of mass conservation between the topics in the absence of document.

[NOTE: what follows in this section will change - it's probably not useful]

To align topics based on the distribution of word probability in these topics, we first define the following concepts:

- the average word frequency: $f_w = \frac{1}{D} \sum_d^D f_{w,d}$ with $f_{w,d} =  \frac{c_{w,d}}{\sum_w^W c_{w,d}}$

- the "topic height": $h_{k^m} = \sum_w^W f_w \ \beta_{w,k}$

- the "reference topic height" in each topic: $h_{k^m, k^R} = \sum_w^{W^R} f_w \  \beta_{w,k^m}$

[These definitions are sufficient to draw the composition of each topic for each $m$, but to draw the flow between the topics, we need to find the optimal mass transfer]


# Implementation

We have implemented the methods described above in a series of functions which can be ran sequentially:

- `run_LDA_models`, which runs the LDA models for a specified set of $K$s, $\alpha$s, modalities or datasets. [NOTE:so far, it's only implemented for K]

- `align_topics` performs the topic alignment on the $\gamma$ and/or the $\beta$ matrices when possible  [NOTE: only the alignment based on $\gamma$ is implemented] and re-order the topics so that most align topics are "close to each others" [not sure how to formulate this]. If a reference model is not provided, the last model is used as a reference.

- `visualize_topic_alignment` computes the visualization layout and return a ggplot object with the alignment flow between topics.

Below is an example of how these functions are used on vaginal microbiome data.

## Example 1: varying $K$

```{r workflow-demo-libraries}

# Libraries to attach
library(tidyverse)
library(magrittr)
library(topicmodels)
library(slam)

# load the topic alignment functions
source("R_LSY/align_topics_all_functions.R")

# viz default theme
theme_set(theme_minimal())

```

```{r workflow-demo-load-and-prepare-data}

load(file = "vm_16s_data.Rdata", verbose = TRUE)
new_asv_names =  colnames(vm_16s) %>% 
  str_split_fixed(., " ", n = 8) %>%
  as.matrix() %>% .[,c(6, 7, 8)]  %>%
  as.data.frame() %>% 
  set_colnames(c("genus","species","strain")) %>% 
   mutate(short_name = 
            str_c(genus, " ", 
                  species %>% str_replace(.,"NA","-")," ",
                  strain)) %>% 
  select(short_name) %>% unlist()

j = which(duplicated(new_asv_names))
new_asv_names[j] = str_c(new_asv_names[j], " (", 1:length(j),")")
colnames(vm_16s) = new_asv_names
  
vm_16s <- slam::as.simple_triplet_matrix(vm_16s %>%  round())


```

```{r workflow-demo-run-lda-models}

topic_models_dir = "lda_models/"

lda_models = 
  run_lda_models(
    data = as.matrix(vm_16s),
    Ks = 1:16,
    method = "VEM",
    seed = 2,
    dir = topic_models_dir
  )

names(lda_models)
head(lda_models$betas[[1]])
head(lda_models$gammas[[1]])

```



```{r workflow-demo-align-topics}
source("R_LSY/align_topics_all_functions.R")
aligned_topics = 
  align_topics(
    lda_models = lda_models
  )

names(aligned_topics)
head(aligned_topics$gamma_alignment)
head(aligned_topics$beta_alignment)
```


```{r workflow-demo-order-topics}

ggplot(aligned_topics$topics_order, aes(x = m, y = k, col = k_LDA)) + 
  geom_text(aes(label = k_LDA)) + guides(col = FALSE)

```


```{r workflow-demo-viz, fig.width=10, fig.height=8}
visualize_aligned_topics(
  aligned_topics = aligned_topics,
  add_leaves = TRUE,
  min_beta = 0.05,
  add_words_labels = TRUE
)

visualize_aligned_topics(
  aligned_topics = aligned_topics,
  add_leaves = TRUE,
  min_beta = 0.05,
  add_words_labels = TRUE,
  method = "beta_alignment"
)

```


Below is a test with an order constrain on the topics of the last model.


```{r}

aligned_topics_with_c = 
  align_topics(
    lda_models = lda_models,
    order_constrain = tibble(m = 16 %>% factor(., levels = 1:16),
                             k_LDA = as.character(1:16),
                             k = c(1,2,3,4,5,9,10,11,12,13,14,17,18, 21,22,23))
  )


g_aligned_topics_with_c = 
  visualize_aligned_topics(
    aligned_topics = aligned_topics_with_c,
    add_leaves = TRUE,
    min_beta = 0.05,
    add_words_labels = TRUE
    )

g_aligned_topics_with_c


```


Checking the alignment scores along the branches (in an attempt to find robust topics or to prune the topic tree when the topics stop being well aligned)


```{r}

scores = compute_score_along_branches(aligned_topics = aligned_topics)

ggplot(scores, 
       aes(x = m %>% as.numeric(), y = score, col = branch %>% factor())) +
  geom_line()


ggplot(scores, 
       aes(x = m %>% as.numeric(), y = score %>% log(), col = branch %>% factor())) +
  geom_line()


ggplot(scores, 
       aes(x = m, y = branch, fill = branch %>% factor(), alpha = score)) +
  geom_tile() +
  scale_alpha(range = c(0,1)) +
  guides(fill = FALSE, alpha = FALSE)



ggplot(scores, 
       aes(x = m, y = branch, fill = branch %>% factor(), alpha = score)) +
  geom_tile() +
  scale_alpha(range = c(0,1)) +
  guides(fill = FALSE, alpha = FALSE)



ggplot(scores, 
       aes(x = m, y = branch, fill = branch %>% factor(), alpha = score %>% log())) +
  geom_tile() +
  scale_alpha(range = c(0,1)) +
  guides(fill = FALSE, alpha = FALSE)


ggplot(scores %>% 
         arrange(branch, m) %>% 
         group_by(branch) %>% 
         mutate(first_m = first(m)) %>% 
         ungroup() %>% 
         arrange(first_m) %>% 
         mutate(branch = branch %>% factor(., unique(branch))), 
       aes(x = m, y = branch, fill = score)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "steelblue")


scores %>% 
  group_by(m) %>% 
  summarize(score = mean(score)) %>% 
  ggplot(aes(x = m %>% as.numeric(), y = score)) + geom_line() +
  scale_x_continuous(breaks = 1:16, minor_breaks = NULL)

```

## Example 2: varying $\alpha$


## Example 3: aligning topic accross modalities



